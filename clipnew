import os import pandas as pd # Define the source and target folders source_folder = 'path/to/source/folder' target_folder = 'path/to/target/folder' # Ensure the target folder exists os.makedirs(target_folder, exist_ok=True) # List all Excel files in the source folder excel_files = [f for f in os.listdir(source_folder) if f.endswith('.xlsx')] # Process each Excel file for excel_file in excel_files: # Construct the full file path file_path = os.path.join(source_folder, excel_file) # Read the "Todays Movements" sheet df = pd.read_excel(file_path, sheet_name='Todays Movements', usecols='I:J') # Save the extracted data to a new Excel file in the target folder output_path = os.path.join(target_folder, excel_file) df.to_excel(output_path, index=False) print("Data extraction complete!"
missing_values = df.select([(count(when(col(c).isNull(), c)) / count(lit(1))).alias(c) for c in df.columns])

# Calculate zeros
zeros = df.select([(count(when(col(c) == 0, c)) / count(lit(1))).alias(c) for c in df.columns])

# Calculate median
medians = df.approxQuantile(df.columns, [0.5], 0.01)
median_values = {col: medians[i][0] for i, col in enumerate(df.columns)}

# Create a DataFrame for medians
median_df = spark.createDataFrame([median_values])

column_name = "your_column_name"

# Calculate missing values
missing_values = df.select((count(when(col(column_name).isNull(), column_name)) / count(lit(1))).alias("missing_values"))

# Calculate zeros
zeros = df.select((count(when(col(column_name) == 0, column_name)) / count(lit(1))).alias("zeros"))

# Calculate mean
mean_value = df.select(mean(col(column_name)).alias("mean"))

# Calculate median
median_value = df.approxQuantile(column_name, [0.5], 0.01)[0]

# Calculate distinct values
distinct_values = df.select(approx_count_distinct(col(column_name)).alias("distinct_values"))

# Calculate null percentage
null_percentage = df.select((count(when(col(column_name).isNull(), column_name)) / count(lit(1)) * 100).alias("null_percentage"))



These scenarios should simulate real-world usage scenarios to ensure thorough testing.

access controls to provide users with an environment that accurately reflects the final system.
 This ensures that users are equipped with the knowledge and tools they need to conduct UAT successfully.
Regression testing
Validation and sign off

Coordination between QE teams and users is essential throughout the UAT process. QE teams should:

Maintain Open Communication: Regularly communicate with users to address questions, concerns, and issues that arise during testing.

Provide Support: Offer assistance and guidance to users throughout the testing process, ensuring they have the resources they need to conduct UAT effectively.

Facilitate Feedback Sessions: Schedule feedback sessions with users to gather input on the testing process, test coverage, and system usability. This feedback helps improve future iterations of the system.

Define the scope of UAT
or success criteria for UAT
User Involvement:
Involve users early in UAT planning.
Regularly communicate progress and gather feedback.
User Training:
Train users on UAT scenarios, tools, and processes.
User Feedback Loop:
Users provide feedback during testing.
QE incorporates feedback into test scenarios.
Collaboration:
QE collaborates with users to understand their needs.
Users and QE jointly prioritize defects.
Documentation and Training Materials:
QE prepares UAT documentation and training materials for users.




Development Environment:
- Python Distribution: Ensure compatibility with Databricks runtime Python version.
- IDE: Use Databricks notebooks or Databricks Connect for local development.
- Version Control: Set up Git repository with appropriate branching for collaboration.

Python Packages:
- Pytest: Testing framework.
- PySpark: Ensure compatibility with Databricks runtime version.
- Pandas: Data manipulation library.
- Great Expectations: Data validation framework.
- Other Dependencies: Verify compatibility with Databricks and Azure DevOps.

Additional Tools:
- Azure DevOps (ADO) Pipeline:
  - Set up CI/CD pipeline in Azure DevOps.
  - Configure tasks for Python scripts and notebooks.
- Databricks:
  - Set up Databricks workspace and cluster.
  - Configure environment variables for secrets or configurations.

Setup:
- Install Packages: Ensure compatibility with Databricks and Azure DevOps environments.
- Set Up IDE: Use Databricks notebooks or Databricks Connect.
- Set Up Git: Establish Git repository for version control.
- Set Up Spark: Follow Databricks documentation for cluster setup.
- Set Up Great Expectations: Follow installation instructions for Databricks environment.
- Document Dependencies: Document compatibility requirements for Databricks and Azure DevOps environments.


Certainly! When creating a test automation framework, you'll need a combination of tools, libraries, and software to ensure efficient development and testing. Here's a list of essential components to consider:

1. **Test Automation Frameworks**:
    - **Selenium**: An open-source framework that allows you to write test scripts in multiple programming languages like Ruby, Java, Node.js, PHP, Perl, Python, JavaScript, C#, and moreÂ¹.
    - **Katalon Studio**: A free and open-source automation testing tool that provides a comprehensive environment for web, mobile, and API testingÂ².

2. **Python Packages**:
    - **Pytest**: A popular testing framework for writing simple and scalable test cases.
    - **Pandas**: A powerful data manipulation library for data analysis.
    - **Great Expectations**: A package for validating data quality and expectations.

3. **IDE (Integrated Development Environment)**:
    - Choose an IDE that supports Python development. Some popular options include:
        - **PyCharm**: A feature-rich IDE with excellent support for Python development.
        - **Visual Studio Code (VS Code)**: A lightweight and customizable code editor with Python extensions.
        - **Jupyter Notebook**: Useful for interactive data analysis and exploration.

4. **Additional Libraries and Tools**:
    - **Requests**: For making HTTP requests (useful for API testing).
    - **BeautifulSoup**: For web scraping and parsing HTML content.
    - **Faker**: For generating fake data during testing.
    - **Coverage**: To measure code coverage during testing.
    - **Git**: Version control system for managing your codebase.
    - **Docker**: For containerization and consistent environments.
    - **Jenkins/Travis CI/CircleCI**: For continuous integration and automated builds.

5. **Database Tools**:
    - If your application interacts with databases:
        - **SQLAlchemy**: For working with databases in Python.
        - **Apache Spark (PySpark)**: If you're dealing with big data and need distributed processing.

6. **Browser Drivers**:
    - If using Selenium, you'll need browser drivers (e.g., ChromeDriver, GeckoDriver) for different browsers.

Remember that this list is not exhaustive, and your specific requirements may vary based on your project. Make sure to install the necessary packages using `pip install package_name` and set up your chosen IDE to get started with your test automation framework. Good luck! ðŸ˜ŠÂ¹Â²

Source: Conversation with Copilot, 6/3/2024
(1) 35 Best Test Automation Frameworks for 2024 - LambdaTest. https://www.lambdatest.com/blog/best-test-automation-frameworks/.
(2) Top 10 free open-source testing tools, Framework & Libraries - Katalon. https://katalon.com/resources-center/blog/open-source-testing-tools.
(3) 9 open source test-automation frameworks | Opensource.com. https://opensource.com/article/20/7/open-source-test-automation-frameworks.
(4) 21 Best Automation Testing Tools in 2024: Ultimate List - phoenixNAP. https://phoenixnap.com/blog/best-automation-testing-tools.
(5) Popular Test Automation Frameworks: How to Choose | BrowserStack. https://www.browserstack.com/guide/best-test-automation-frameworks.
