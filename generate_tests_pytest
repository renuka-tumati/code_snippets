import pytest
import pandas as pd
import numpy as np
import pyodbc
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Define fixtures for SSMS and Databricks connections
@pytest.fixture(scope='module')
def ssms_connection():
    conn_str = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=your_server;DATABASE=your_database;UID=your_username;PWD=your_password'
    connection = pyodbc.connect(conn_str)
    yield connection
    connection.close()

@pytest.fixture(scope='module')
def databricks_spark_session():
    spark = SparkSession.builder \
        .appName("pytest") \
        .getOrCreate()
    yield spark
    spark.stop()

# Helper function to execute queries and return DataFrame
def execute_query_to_df(connection, query, is_spark=False):
    if is_spark:
        return connection.sql(query).toPandas()
    else:
        return pd.read_sql(query, connection)

# Test function to compare distinct values
def test_distinct_values(ssms_connection, databricks_spark_session):
    # Define queries
    ssms_query = "SELECT DISTINCT column_name FROM source_table"
    databricks_query = "SELECT DISTINCT column_name FROM target_table"

    # Execute queries
    ssms_df = execute_query_to_df(ssms_connection, ssms_query)
    databricks_df = execute_query_to_df(databricks_spark_session, databricks_query, is_spark=True)

    # Normalize columns and replace blanks with NaNs
    ssms_df['column_name'] = ssms_df['column_name'].replace('', np.nan)
    databricks_df['column_name'] = databricks_df['column_name'].replace('', np.nan)

    # Compute distinct counts
    ssms_distinct_count = ssms_df['column_name'].dropna().nunique()
    databricks_distinct_count = databricks_df['column_name'].dropna().nunique()

    # Assert distinct counts match
    assert ssms_distinct_count == databricks_distinct_count, "Distinct counts do not match"

    # Compute and compare distinct values
    ssms_distinct_values = set(ssms_df['column_name'].dropna())
    databricks_distinct_values = set(databricks_df['column_name'].dropna())

    # Assert distinct values match
    assert ssms_distinct_values == databricks_distinct_values, "Distinct values do not match"

    # Print the results (optional for debugging purposes)
    print(f"SSMS Distinct Count: {ssms_distinct_count}")
    print(f"Databricks Distinct Count: {databricks_distinct_count}")
    print(f"SSMS Distinct Values: {ssms_distinct_values}")
    print(f"Databricks Distinct Values: {databricks_distinct_values}")




# Filter rows to exclude those where both 'value1' and 'value2' are NaN
filtered_df = merged_df[~(merged_df['value1'].isna() & merged_df['value2'].isna())]

print("\nFiltered DataFrame (excluding rows where both sides are NaN):")
print(filtered_df)

# Perform the comparison to find mismatches
# Check where 'value1' is not equal to 'value2', and ensure at least one is not NaN
filtered_df['mismatch'] = (filtered_df['value1'] != filtered_df['value2'])

# Get the rows with mismatches
mismatched_rows = filtered_df[filtered_df['mismatch']]



import pandas as pd
import numpy as np

# Example DataFrames
df1 = pd.DataFrame({
    'value': [10.0, 20.0, 30.0]
})

df2 = pd.DataFrame({
    'value': [10.00001, 20.0, 30.0]
})

# Ensure columns are of the same type
df1['value'] = df1['value'].astype(float)
df2['value'] = df2['value'].astype(float)

# Compare using a tolerance level for floating-point numbers
tolerance = 1e-5
comparison = np.isclose(df1['value'], df2['value'], atol=tolerance)

# Print comparison results
print("Comparison results:")
print("Matches:")
print(df1[comparison])
print(df2[comparison])

print("\nDifferences:")
diff_indices = ~comparison
print("Differences in df1:")
print(df1[diff_indices])
print("Differences in df2:")
print(df2[diff_indices])


import pandas as pd
import numpy as np

def convert_data_types(df, column_data_types):
    for column, dtype in column_data_types.items():
        if dtype == 'float':
            df[column] = pd.to_numeric(df[column], errors='coerce')
        elif dtype == 'int':
            df[column] = pd.to_numeric(df[column], errors='coerce').astype('Int64')  # Using 'Int64' to handle NaN values
        elif dtype == 'str':
            df[column] = df[column].astype(str)
        elif dtype == 'datetime':
            df[column] = pd.to_datetime(df[column], errors='coerce')
    return df

# Define column data types for both DataFrames
source_column_types = {
    'source_col1': 'float',
    'source_col2': 'str',
    # Add all columns with their respective types
}

target_column_types = {
    'target_col1': 'float',
    'target_col2': 'str',
    # Add all columns with their respective types
}

# Fetch data from source and target tables
source_df = pd.read_sql(source_query, engine)
target_df = pd.read_sql(target_query, engine)

# Convert data types
source_df = convert_data_types(source_df, source_column_types)
target_df = convert_data_types(target_df, target_column_types)



{
    "source_table": "source_table_name",
    "target_table": "target_table_name",
    "source_columns": [
        "source_col1",
        "source_col2",
        "source_col3",
        // Add all your source columns
    ],
    "target_columns": [
        "target_col1",
        "target_col2",
        "target_col3",
        // Add all your target columns
    ],
    "as_of_dates": [
        "2024-01-01",
        "2024-02-01",
        "2024-03-01",
        // Add all your dates
    ]
}



    # Ensure source and target columns lists are of the same length
    if len(source_columns) != len(target_columns):
        raise ValueError("The number of source columns must match the number of target columns.")
    
    for src_col, tgt_col in zip(source_columns, target_columns):
        for date in as_of_dates:
            test_cases.append((source_table, target_table, src_col, tgt_col, date))
    
    return test_cases


import pytest
import pandas as pd
import json
from sqlalchemy import create_engine

# Database connection parameters
DATABASE_URL = "your_database_connection_string"
engine = create_engine(DATABASE_URL)

# Load configuration from file
def load_config():
    with open('config.json', 'r') as file:
        return json.load(file)

config = load_config()

# Generate test cases based on the loaded configuration
def generate_test_cases(config):
    test_cases = []
    source_table = config["source_table"]
    target_table = config["target_table"]
    source_columns = config["source_columns"]
    target_columns = config["target_columns"]
    as_of_dates = config["as_of_dates"]
    
    for source_col in source_columns:
        for target_col in target_columns:
            for date in as_of_dates:
                test_cases.append((source_table, target_table, source_col, target_col, date))
    return test_cases

# Parameterize the test with all combinations of columns and dates
@pytest.mark.parametrize("source_table, target_table, source_col_name, target_col_name, as_of_date", generate_test_cases(config))
def test_validate_data(source_table, target_table, source_col_name, target_col_name, as_of_date):
    # Build source and target queries
    source_query = f"""
    SELECT ID, {source_col_name}
    FROM {source_table}
    WHERE as_of_date = '{as_of_date}'
    """
    
    target_query = f"""
    SELECT ID, {target_col_name}
    FROM {target_table}
    WHERE as_of_date = '{as_of_date}'
      AND {target_col_name} IS NOT NULL
    """
    
    # Fetch data from source and target tables
    source_df = pd.read_sql(source_query, engine)
    target_df = pd.read_sql(target_query, engine)
    
    # Perform the merge to find differences
    merged_df = pd.merge(source_df, target_df, on="ID", how="outer", indicator=True)
    
    # Find differences
    only_in_source = merged_df[merged_df['_merge'] == 'left_only']
    only_in_target = merged_df[merged_df['_merge'] == 'right_only']
    
    # Assertions
    assert only_in_source.empty, f"Rows in source but not in target: {only_in_source}"
    assert only_in_target.empty, f"Rows in target but not in source: {only_in_target}"
